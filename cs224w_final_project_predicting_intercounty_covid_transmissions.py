# -*- coding: utf-8 -*-
"""CS224W Final Project: Predicting InterCounty COVID Transmissions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y3ArBhyKKTL0QelOBM-X2gPAgWwiA7Fa

# **CS224W Final Project: Predicting InterCounty COVID Transmissions**
*Authors: Michelle Qin, Stephan Sharkov, David Witten*

# **Import all required libraries and connect to gdrive**
"""

# Install all torch extension libraries for graph manipulation
import os
if 'IS_GRADESCOPE_ENV' not in os.environ:
  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html
  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html
  !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html
  !pip install torch-geometric
  !pip install -q git+https://github.com/snap-stanford/deepsnap.git

# Import the libraries from above
import torch_geometric
torch_geometric.__version__
import torch_scatter
from torch_geometric.data import Data, DataLoader, Dataset
from torch_geometric.nn.conv import MessagePassing

# Import other libraries for data processing, training, and metrics
import pandas as pd
import pickle
import copy
import matplotlib.pyplot as plt
import torch.optim as optim
import seaborn as sn
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Parameter, Linear
from tqdm import trange
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score
from google.colab import drive
drive.mount('/drive')

# Prepare code to connect to cuda for google cloud deployment
# if torch.cuda.is_available():
# dev = "cuda:0"
#else:
dev = "cpu"
device = torch.device(dev)

"""# Node Classificaton with GraphSAGE and Graph Convolutional Network
Our objective is to help identify and reduce the impact of COVID-19 spread in the United States at a more local level. Previous literature has focused on COVID-19 transmissions between states and countries, but there has been little research done on the impact of transmissions at the county level. We believe understanding the spread of COVID-19 at the county level could better direct COVID-19 mitigation efforts to the local areas that are predicted to be the most affected. **This project explores the use of Graph Neural Networks (GNNs) to analyze and predict intercounty COVID-19 spread via data on the physical proximity of counties as well as data on commute flow between counties.** In particular, we explore the **GraphSAGE** and **Graph Convolutional Network (GCN)** models. 


**The task we are concerned with is node classification for a fully connected graph where the nodes are counties and the edges are weighted by the sum of the inverse of the distance between two counties, plus data on the commute flow between the two counties.** We plan to calculate the distance between counties using their latitude and longitude coordinates. We label the nodes by the risk level compared to the rest of the county on that day. By looking at labels of neighbors (i.e. other counties) with emphasis on neighbors that are closer in distance and have greater amounts of people traveling between them (hence, greater potential for COVID-19 transmissions), our GNN model will determine the label of any given county, that is, the level of COVID-19 risk in that county.

#**Processing data from the datasets.**
**Run ONLY if need to regenerate data otherwise skip until "Loading..."**

This block reads the data of the csv files made by processing excel files from US Census Bureau and NYT github files.

First, we used a **dataset prepared by The New York Times which has information on the number of daily cases in each county in the United States beginning on January 1, 2021.** The dataset consists of the date, name of the county, state, FIPS code of the state, and the number of COVID cases. We are planning to use all the **2021 and first half of 2022 data for training, the second half of 2022 for validation, and all currently available 2023 data for testing**. This approach will allow us to analyze historical COVID trends and make predictions for the future. This data will be used to create node features for each county in our graph. 

Second, based on the nature of COVID-19 transmissions, we understand physical proximity and human contact are causes of spread, and we include this data as edge weights in our graph. **Regarding physical proximity between counties, we will use a dataset on coordinate positions of counties in the United States from opendatasoft**, which consists of the state, FIPS code of the state, coordinates, and name of the county. This data allows us to assign weights to the graph's edges such that we can have higher weights for counties that are closer to each other. This is important to consider as distance represents the possibility of human interaction and travel between two places, as we believe people are less likely to interact in person or travel across faraway counties. 

Third, to add another factor to the weights of our graph's edges, we will use **a dataset on commute flow between counties in the United States from US Census Bureau.** Every row of the dataset contains the FIPS code, state, and county name for each of the two counties in comparison, as well as data on the number of workers in the commuting flow. This will allow us to consider the impact of human movement and interactions on COVID-19 transmissions through analysis on COVID data and land commute between counties. 

Fourth, in order to create node labels, we need to know the population data of each county. Our node labels are representations of the risk level (such as "low", "moderate", or "high") of COVID-19 infection in a given county, which are typically categorized by the number of COVID cases per 100,000 residents. **For all the nodes in our datasets, we utilize the most recent county population totals provided by the US Census Bureau in 2021**.
"""

df_edge = pd.read_csv(r'/drive/Shared drives/cs224w-master-drive/datasets/edge_data2.csv')
print("Traffic and Distance between counties:")
print(df_edge)

county_pop_df = pd.read_csv(r'/drive/Shared drives/cs224w-master-drive/datasets/all_county_pop_data.csv').drop_duplicates()

# Every block that processas cases data is joined with population data, to ensure the overlap of counties
df_train = pd.read_csv(r'/drive/Shared drives/cs224w-master-drive/datasets/training_dataset.csv')
df_train = df_train.dropna()
df_train = df_train.reset_index(drop=True)
merged_df = pd.merge(df_train, county_pop_df, on=['county', 'state'], how='left')
population_df = merged_df.dropna(subset=['population'])
df_train = population_df.reset_index(drop=True)

df_val = pd.read_csv(r'/drive/Shared drives/cs224w-master-drive/datasets/validation_dataset.csv')
df_val = df_val.dropna()
df_val = df_val.drop(labels = df_val.index[df_val['fips'] == 48999].tolist(), axis = 0) 
df_val = df_val.reset_index(drop=True)
merged_df = pd.merge(df_val, county_pop_df, on=['county', 'state'], how='left')
population_df = merged_df.dropna(subset=['population'])
df_val = population_df.reset_index(drop=True)

df_test = pd.read_csv(r'/drive/Shared drives/cs224w-master-drive/datasets/testing_dataset.csv')
df_test = df_test.dropna()
df_test = df_test.drop(labels = df_test.index[df_test['fips'] == 48999].tolist(), axis = 0) 
df_test = df_test.reset_index(drop=True)
merged_df = pd.merge(df_test, county_pop_df, on=['county', 'state'], how='left')
population_df = merged_df.dropna(subset=['population'])
df_test = population_df.reset_index(drop=True)

print("Covid and deaths cases by county training data:") 
print(df_train)

"""This block replaces the fips code with the node number from 0 to 3134 (number of counties), and makes a dictionary mapping for that."""

# First for the training data
fips_n = {}
j = 0   
for idx, i in enumerate(df_train['fips']):
  if i not in fips_n:
    df_train.at[idx, 'fips'] = j
    fips_n[i] = j
    j += 1      
  else:
    df_train.at[idx, 'fips'] = fips_n[i]
print("Replaced the fips codes for the covid data")  
print(df_train) 

# Same for validation data
for idx, i in enumerate(df_val['fips']):
  if i not in fips_n:
    df_val.at[idx, 'fips'] = j
    fips_n[i] = j
    j += 1      
  else:
    df_val.at[idx, 'fips'] = fips_n[i]
print(df_val) 

# And testing data
for idx, i in enumerate(df_test['fips']):
  if i not in fips_n:
    df_test.at[idx, 'fips'] = j
    fips_n[i] = j
    j += 1      
  else:
    df_test.at[idx, 'fips'] = fips_n[i]
print(df_test) 

# At last for for commute traffic and distance data, but dropping the indices that were not in the covid data
indices_drop = []
for col in ['county1', 'county2']:
  for idx, i in enumerate(df_edge[col]):
    if i not in fips_n:
      indices_drop.append(idx)
    else:  
      df_edge.at[idx, col] = fips_n[i]   
df_edge = df_edge.drop(indices_drop) 
df_edge = df_edge.reset_index(drop=True)
print("Replaced the fips codes for the edge data")            
print(df_edge) 
print("Number of edges that don't have covid data due to not full information available: ", len(indices_drop)) #indices that are not in covid data

"""This block moves from Pandas to torch tensors for all important columns that are required for training"""

wicf_edge = torch.tensor(df_edge['wicf'].values, dtype=torch.float32)
wicf_edge = wicf_edge.reshape(wicf_edge.shape[0], 1)
distance_edge = torch.tensor(df_edge['distance'].values, dtype=torch.float32)
distance_edge = distance_edge.reshape(distance_edge.shape[0], 1)
print("Shapes of traffic and distance tensors: ", wicf_edge.shape, " ", distance_edge.shape)

fips1_edge = torch.tensor(df_edge['county1'].values)
fips1_edge = fips1_edge.reshape(fips1_edge.shape[0], 1)
fips2_edge = torch.tensor(df_edge['county2'].values)
fips2_edge = fips2_edge.reshape(fips1_edge.shape[0], 1)
print("Shape of node edge indices for the edge_index: ", fips1_edge.shape, " ", fips2_edge.shape)

# COVID data required extra cleaning, as some counties had duplicate entries for population due to same name county present in a state (e.g. Alabama county in state of Alabama)
date_train = torch.tensor(df_train['date'].values)
fips_train = torch.tensor(df_train['fips'].values, dtype=torch.float32)
cases_train = torch.tensor(df_train['cases'].values, dtype=torch.float32)
population_train = torch.tensor(df_train[['fips', 'population']].drop_duplicates()['population'].values, dtype=torch.float32)
print(date_train.shape, fips_train.shape, cases_train.shape)
print(population_train.shape)

date_val = torch.tensor(df_val['date'].values)
fips_val = torch.tensor(df_val['fips'].values, dtype=torch.float32)
cases_val = torch.tensor(df_val['cases'].values, dtype=torch.float32)
population_val = torch.tensor(df_val[['fips', 'population']].drop_duplicates()['population'].values, dtype=torch.float32)

date_test = torch.tensor(df_test['date'].values)
fips_test = torch.tensor(df_test['fips'].values, dtype=torch.float32)
cases_test = torch.tensor(df_test['cases'].values, dtype=torch.float32)
population_test = torch.tensor(df_test[['fips', 'population']].drop_duplicates()['population'].values, dtype=torch.float32)

# Saving population data
dbfile = open('population_info', 'ab')
pickle.dump((population_train, population_val, population_test), dbfile)                     
dbfile.close()

"""Finalizing edge data by creating edge index tensors and normalized edge embeddings, or unnormalized edge embeddings to try out both."""

edge_index = torch.cat([fips1_edge, fips2_edge], 1).T
print("Size of edge_index: ", edge_index.shape)

wicf_normalized = torch.nn.functional.normalize(input = wicf_edge, dim = 0)
distance_normalized = torch.nn.functional.normalize(input = 1/distance_edge, dim = 0)
edge_attribute = torch.cat([wicf_normalized, distance_normalized], 1)
# edge_attribute = torch.cat([wicf_edge, 1/distance_edge], 1) # Alternative code for using unnormalized edge data in which case comment out the 3 lines before
print("Shape of normalized edge attibutes tensor: ", edge_attribute.shape)
      
# Saving edge data
dbfile = open('edge_info', 'ab')
pickle.dump((edge_attribute, edge_index), dbfile)                     
dbfile.close()
print("Saved to pickle")

"""Creating node embeddings for training data, starting with date map to sequential natural numbers to create a date to tensor row mapping.

For every dataset, gets the cases data that is available for every county within the dataset, and padds 0s into missing days. This is because we assume there were no covid cases there if the date is missing.
"""

date_row_train = {}
index = 0
for date in date_train.unique():
  date_row_train[date.item()] = index
  index += 1
print("Training date dictionary: ", date_row_train)

#F irst for training data
data_traing_pren = []
k = 0
for county in fips_train.unique(): # We only need to iterate through counties but right now they are repeated for daily data
  indices = (fips_train == county).nonzero()
  case_data = cases_train[indices]
  date_data = [date_row_train[i[0].item()] for i in date_train[indices]]
  date_daat = torch.Tensor(date_data).to(torch.int64) # Get the indices that need to get real data in them
  final_data = torch.zeros((date_train.unique().shape[0], 1), dtype=torch.float32) # Zero tensor to padd for missing days
  final_data[date_daat] = case_data
  data_traing_pren.append(final_data) # we append every column (every county data) to a list to have days as rows later
fdata_train = torch.cat(data_traing_pren, dim=1)   
train_data_norm = fdata_train.reshape((fdata_train.shape[0], fdata_train.shape[1], 1))
print("Training data shape: ", train_data_norm.shape)

# Same for validation data
date_row_val = {}
index = 0
for date in date_val.unique():
  date_row_val[date.item()] = index
  index += 1
print("Validation date dictionary: ", date_row_val)

data_val_pren = []
k = 0
for county in fips_val.unique(): # We only need to iterate through counties but right now they are repeated for daily data
  indices = (fips_val == county).nonzero()
  case_data = cases_val[indices]
  date_data = [date_row_val[i[0].item()] for i in date_val[indices]]
  date_daat = torch.Tensor(date_data).to(torch.int64) # Get the indices that need to get real data in them
  final_data = torch.zeros((date_val.unique().shape[0], 1), dtype=torch.float32) # Zero tensor to padd for missing days
  final_data[date_daat] = case_data
  data_val_pren.append(final_data) # We append every column (every county data) to a list to have days as rows later
fdata_val = torch.cat(data_val_pren, dim=1)   
val_data_norm = fdata_val.reshape((fdata_val.shape[0], fdata_val.shape[1], 1))
print("Validation data shape: ", val_data_norm.shape)

# Same for testing data
date_row_test = {}
index = 0
for date in date_test.unique():
  date_row_test[date.item()] = index
  index += 1
print("Testing date dictionary: ", date_row_test)

data_test_pren = []
k = 0
for county in fips_test.unique(): # We only need to iterate through counties but right now they are repeated for daily data
  indices = (fips_test == county).nonzero()
  case_data = cases_test[indices]
  date_data = [date_row_test[i[0].item()] for i in date_test[indices]]
  date_daat = torch.Tensor(date_data).to(torch.int64) # Get the indices that need to get real data in them
  final_data = torch.zeros((date_test.unique().shape[0], 1), dtype=torch.float32) # Zero tensor to padd for missing days
  final_data[date_daat] = case_data
  data_test_pren.append(final_data) # We append every column (every county data) to a list to have days as rows later
fdata_test = torch.cat(data_test_pren, dim=1)   
test_data_norm = fdata_test.reshape((fdata_test.shape[0], fdata_test.shape[1], 1))
print("Testing data shape: ", test_data_norm.shape)

# Saving all three sets of embeddings
dbfile = open('node_info_unnorm', 'ab')
pickle.dump((train_data_norm, val_data_norm, test_data_norm), dbfile)                     
dbfile.close()

"""# **Loading the data and preparing for model training**

Loading all the important pickle data that was saved before, to avoid rerunning the blocks above, taking RAM and time. 

Putting this data onto device for CUDA on Google Cloud.
"""

# Edge information
dbfile = open(r'/drive/Shared drives/cs224w-master-drive/edge_info', 'rb')
(edge_attribute, edge_index) = pickle.load(dbfile)
dbfile.close()
print("Checking that shape matches: ", edge_index.shape)
edge_attribute = torch.ones(edge_attribute.shape)
edge_index.to(device)
edge_attribute.to(device)

# Node information
dbfile = open(r'/drive/Shared drives/cs224w-master-drive/node_info_unnorm', 'rb')
(train_data_norm, val_data_norm, test_data_norm) = pickle.load(dbfile)
dbfile.close()
print("Checking that shape matches: ", train_data_norm.shape)
train_data_norm.to(device)
val_data_norm.to(device)
test_data_norm.to(device)

# Population information
dbfile = open(r'/drive/Shared drives/cs224w-master-drive/population_info', 'rb')
(population_train, population_val, population_test) = pickle.load(dbfile)
dbfile.close()
print("Checking that shape matches: ", population_train.shape)
population_train.to(device)
population_val.to(device)
population_test.to(device)

"""This block finalizes the Data objects by forming the node lables for each dataset, either for 4 class or 2 class classification problem. **The labels are formed based on which quantile (or median side for 2 class) the datapoint is at, where the datapoint is also divided by the population of the county to get the relative COVID exposure in the county.** All the loaded data above is put into the Data objects, where each Data object contains data for one day for all counties, and stores the connectivity of the counties.

Current code is for 2 class problem, commented code is for 4 class problem.
"""

data_pyg_train = []
for i in range (0, train_data_norm.shape[0]): 
  tdata_over_pop = train_data_norm[i].squeeze(1) / population_train
  node_labels = torch.zeros((train_data_norm[i].shape[0]))
  node_labels[tdata_over_pop < torch.median(tdata_over_pop).item()] = 0
  node_labels[tdata_over_pop >= torch.median(tdata_over_pop).item()] = 1
  # quanty = torch.quantile(tdata_over_pop, [0.25, 0.5, 0.75], dim=0, keepdim=True, interpolation='linear')
  # first_q = tdata_over_pop <= quanty[0]
  # second_q = (tdata_over_pop > quanty[0]) & (tdata_over_pop <= quanty[1])
  # third_q = (tdata_over_pop > quanty[1]) & (tdata_over_pop <= quanty[2])
  # fourth_q = tdata_over_pop > quanty[2]
  # node_labels[first_q] = 0
  # node_labels[second_q] = 1
  # node_labels[third_q] = 2
  # node_labels[fourth_q] = 3
  data_pyg_train.append(Data(x=train_data_norm[i], edge_index=edge_index, edge_attr = edge_attribute, y = node_labels))
print("Done with training data")

data_pyg_val = []
for i in range (0, val_data_norm.shape[0]):
  vdata_over_pop = val_data_norm[i].squeeze(1) / population_val
  node_labels = torch.zeros((val_data_norm[i].shape[0]))
  node_labels[vdata_over_pop < torch.median(vdata_over_pop).item()] = 0
  node_labels[vdata_over_pop >= torch.median(vdata_over_pop).item()] = 1
  # quanty = torch.quantile(vdata_over_pop, [0.25, 0.5, 0.75], dim=0, keepdim=True, interpolation='linear')
  # first_q = vdata_over_pop <= quanty[0]
  # second_q = (vdata_over_pop > quanty[0]) & (vdata_over_pop <= quanty[1])
  # third_q = (vdata_over_pop > quanty[1]) & (vdata_over_pop <= quanty[2])
  # fourth_q = vdata_over_pop > quanty[2]
  # node_labels[first_q] = 0
  # node_labels[second_q] = 1
  # node_labels[third_q] = 2
  # node_labels[fourth_q] = 3
  data_pyg_val.append(Data(x=val_data_norm[i], edge_index=edge_index, edge_attr = edge_attribute, y = node_labels))
print("Done with val data")

data_pyg_test = []
for i in range (0, test_data_norm.shape[0]):
  ttdata_over_pop = test_data_norm[i].squeeze(1) / population_test
  node_labels = torch.zeros((test_data_norm[i].shape[0]))
  node_labels[ttdata_over_pop < torch.median(ttdata_over_pop).item()] = 0
  node_labels[ttdata_over_pop >= torch.median(ttdata_over_pop).item()] = 1
  # quanty = torch.quantile(ttdata_over_pop, [0.25, 0.5, 0.75], dim=0, keepdim=True, interpolation='linear')
  # first_q = ttdata_over_pop <= quanty[0]
  # second_q = (ttdata_over_pop > quanty[0]) & (ttdata_over_pop <= quanty[1])
  # third_q = (ttdata_over_pop > quanty[1]) & (ttdata_over_pop <= quanty[2])
  # fourth_q = ttdata_over_pop > quanty[2]
  # node_labels[first_q] = 0
  # node_labels[second_q] = 1
  # node_labels[third_q] = 2
  # node_labels[fourth_q] = 3
  data_pyg_test.append(Data(x=test_data_norm[i], edge_index=edge_index, edge_attr = edge_attribute, y = node_labels))
print("Done with test data")

"""# **Training Models for Node Classification**

We will now use our 2 models to complete node classification on the dataset. In our dataset, nodes correspond to counties with their COVID data, and edges correspond to connections between counties, distance and commute flow. Each node or county in the graph is assigned a class label based on the COVID cases relative to other counties in the country. The graph has 3134 nodes, 414361 edges, 2 or 4 prediction classes.

# **Training a GraphSAGE model**

For our first model, we use GraphSage([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)).

This model code and some part of documentation was provided by CS224W Colab assignments. 

We start with a general Graph Neural Network Stack, into which we will be able to plugin our own module implementations, in this case GraphSAGE.

**GNN Stack Module**

Below is the implementation of a general GNN stack. It has 2 layers that combine GraphSAGE convolutions, ReLU and Dropout. Both layers expand the embedding size to learn the differences between the embeddings. After both layers two linear modules are applied to the input with Dropout in between, so we can get to the number of classes we need (output_dim), before applying softmax to get the probabilties of the node belonging to each of the classes. We use negative log likelihood as our loss function.
"""

class GNNStack(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):
        super(GNNStack, self).__init__()
        conv_model = self.build_conv_model(args.model_type)
        self.convs = nn.ModuleList() # Creates the list of all the GraphSAGE models we will apply to the input
        self.convs.append(conv_model(input_dim, hidden_dim))
        assert (args.num_layers >= 1), 'Number of layers is not >=1'
        for l in range(args.num_layers-1):
            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))

        self.post_mp = nn.Sequential( # Post Message Passing modules for finalizing the predictions
            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), 
            nn.Linear(hidden_dim, output_dim))

        self.dropout = args.dropout
        self.num_layers = args.num_layers

        self.emb = emb # Boolean variable to ouput embeddings if needed

    def build_conv_model(self, model_type):
        return GraphSage

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
          
        for i in range(self.num_layers):
            x = self.convs[i](x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout,training=self.training)

        x = self.post_mp(x)

        if self.emb == True:
            return x

        return F.log_softmax(x, dim=1)

    def loss(self, pred, label):
        return F.nll_loss(pred, label)

"""**GraphSage Implementation**


For a given *central* node $v$ with current embedding $h_v^{l-1}$, the message passing update rule to tranform $h_v^{l-1} \rightarrow h_v^l$ is as follows: 

\begin{equation}
h_v^{(l)} = W_l\cdot h_v^{(l-1)} + W_r \cdot AGG(\{h_u^{(l-1)}, \forall u \in N(v) \})
\end{equation}

where $W_1$ and $W_2$ are learanble weight matrices and the nodes $u$ are *neighboring* nodes. Additionally, we use mean aggregation for simplicity:

\begin{equation}
AGG(\{h_u^{(l-1)}, \forall u \in N(v) \}) = \frac{1}{|N(v)|} \sum_{u\in N(v)} h_u^{(l-1)}
\end{equation}

One thing to note is that we're adding a **skip connection** to our GraphSage implementation through the term $W_l\cdot h_v^{(l-1)}$. 

Our `forward` function propagates the messages from all nodes to their neighbors, and normalizes the combination of aggregated neighbor messages, and skip-connection of the nodes data.

Our `message` function just passes the node embedding of each neighbor.

Our `aggregate` function utilizes `torch_scatter.scatter` to combine all the neihgboring embeddings.

"""

class GraphSage(MessagePassing):
    def __init__(self, in_channels, out_channels, normalize = True,
                 bias = False, **kwargs):  
        super(GraphSage, self).__init__(**kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalize = normalize

        self.lin_l = Linear(in_channels, out_channels, bias=bias) # Weight matrices for skip-conection and aggregated messages
        self.lin_r = Linear(in_channels, out_channels, bias=bias)

        self.reset_parameters()

    def reset_parameters(self):
        self.lin_l.reset_parameters()
        self.lin_r.reset_parameters()

    # Calling propagate on our x to pass and aggregate the messages, as well as normalize the passed info
    def forward(self, x, edge_index, size = None):
        out = self.propagate(edge_index, x=(x,x), size=size) 
        if self.normalize:
          out = F.normalize(self.lin_r(out.to(torch.float32)) + self.lin_l(x.to(torch.float32)), p=2)
        return out

    # Computes messages for each neighbor j
    def message(self, x_j): 
        return x_j

    # Mean Aggregation of all the messages that are passed as input
    def aggregate(self, inputs, index, dim_size = None): 
        return torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce="mean")

"""**GraphSage Training**

In this block we define training and testing functions. We use Adam optimizer, due to it showing better performance on most of Graph related tasks, especially node classificaiton. We check performance of our model on validation set at every epoch, as we don't have many epochs we iterate over. And for validation test we only work with accuracy metric to determine the best model, whereas for final testing dataset we print out performance based on other metrics: F1-score, Precision, Recall, and ROC-AUC.
"""

class objectview(object):
    def __init__(self, d):
        self.__dict__ = d

def train(args):
    model = GNNStack(data_pyg_train[0].num_node_features, args.hidden_dim, 2, 
                                args)
    model.to(device) # Move to cuda for google cloud training

    filter_fn = filter(lambda p : p.requires_grad, model.parameters())
    opt = optim.Adam(filter_fn, lr=args.lr, weight_decay=args.weight_decay)
    losses = []
    test_accs = []
    best_acc = 0
    best_model = None
    for epoch in trange(args.epochs, desc="Training", unit="Epochs"):
        total_loss = 0
        model.train()

        for batch in train_loader:
            opt.zero_grad()
            pred = model(batch)
            label = batch.y
            loss = model.loss(pred, label.to(torch.int64))
            loss.backward()
            opt.step()
            total_loss += loss.item() * batch.num_graphs
        total_loss /= len(train_loader.dataset)
        losses.append(total_loss)

        # Test the performance on our validation dataset
        test_acc = test(val_loader, model, is_validation=True)
        test_accs.append(test_acc)
        if test_acc >= best_acc:
          best_acc = test_acc
          best_model = copy.deepcopy(model) 
    return test_accs, losses, best_model, best_acc

def test(loader, test_model, is_validation=False, model_type=None):
    # Need to collect predictions and labels across the loader to compute important metrics for the test data
    test_model.eval()
    all_label_list = []
    all_pred_list = []
    all_rocpred_list = []
    correct = 0
    for data in loader:
        with torch.no_grad():
            pred2 = test_model(data)
            pred = pred2.max(dim=1)[1]
            label = data.y
            
        correct += pred.eq(label).sum().item()
        all_label_list.append(label)
        all_pred_list.append(pred)
        all_rocpred_list.append(pred2)
    final_l = torch.cat(all_label_list)
    final_p = torch.cat(all_pred_list)
    final_p2 = torch.cat(all_rocpred_list)

    # Calculates alternative metrics, current code is for 2 class prediction, commented code is for 4 class prediction
    if not is_validation:
      print("F1-score", f1_score(final_l, final_p, average='macro'))
      print("Precision", precision_score(final_l, final_p, average='macro'))
      print("Recall", recall_score(final_l, final_p, average='macro'))
      print("Roc-auc-score", roc_auc_score(final_l, final_p))
      metric = confusion_matrix(final_l, final_p)
      df_cm = pd.DataFrame(metric/ 1000, index = [0,1],
                    columns = [0,1])
      # print("Roc-auc-score", roc_auc_score(final_l, (final_p2.T/final_p2.sum(dim=1)).T, average='macro', multi_class='ovo'))
      # metric = confusion_matrix(final_l, final_p)
      # df_cm = pd.DataFrame(metric/ 1000, index = [0,1,2,3],
      #             columns = [0,1,2,3])
      plt.figure(figsize = (3,3))
      sn.heatmap(df_cm, annot=True)
      plt.show()
      plt.savefig('confusion_4.png')

    # Calculate and return accuracy
    total = 0
    for data in loader.dataset:
        total += data.x.shape[0]

    return correct / total

#Define important arguments and hyperparameters
args = {'model_type': 'GraphSage', 'num_layers': 2, 'heads': 1, 'batch_size': 16, 'hidden_dim': 32, 'dropout': 0.3, 'epochs': 5, 'opt': 'adam', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01}
args = objectview(args)
model = 'GraphSage'
args.model_type = model

#Load the data into loaders 
train_loader = DataLoader(data_pyg_train, batch_size = args.batch_size)
val_loader = DataLoader(data_pyg_val, batch_size = args.batch_size)
test_loader = DataLoader(data_pyg_test, batch_size = args.batch_size)

#Train and print the validation results, along with training loss graph
test_accs, losses, best_model, best_acc = train(args) 
print("Maximum val set accuracy: {0}".format(max(test_accs)))
print("Minimum loss: {0}".format(min(losses)))
plt.title("Accuracy trial")
plt.plot(losses, label="Training loss" + " - " + args.model_type)
plt.plot(test_accs, label="Validation accuracy" + " - " + args.model_type)
plt.legend()
plt.show()
plt.savefig('4_class_unnorm.png')

# Run test for our best model to get results with other metrics
final_test_acc = test(test_loader, best_model, is_validation=False, model_type=model)
print("Final Test Accuracy: ", final_test_acc)

"""# **Training a MessagePassing GCN model**  (DAVID)

# **Appendix**

This is some of our other code we used for data exploration, as well as pre-processing that gets pandas dataframes, but is not responsible for any feature creation process like the code at the beginning of the colab.
"""

# Print statistics on the covid cases data ("Run ONLY..." or "Loading..." block has to be run before this)
print("Mean Train", torch.mean(train_data_norm).item())
print("Median Train", torch.median(train_data_norm).item())
print("Max Train", torch.max(train_data_norm).item())
print("Mean Val", torch.mean(val_data_norm).item())
print("Median Val", torch.median(val_data_norm).item())
print("Max Val", torch.max(val_data_norm).item())
print("Mean Test", torch.mean(test_data_norm).item())
print("Median Test", torch.median(test_data_norm).item())
print("Max Test", torch.max(test_data_norm).item())

# Creates the edge_data file that contains commute traffic and distance data for each county pair
import csv
from geopy import distance

# Calculate distances
def create_distances():
    hm = {}
    def distances(a,b):
        return distance.great_circle(hm[a], hm[b]).km # this is 20x faster for only slightly more error

    with open("us-county-boundaries.csv", 'r') as f:
        first = True
        f.readline() # gets rid of header
        for i in f:
            row = i.split(";")
            hm[row[5]] = row[0]
    hm2 = {}
    counties = sorted(hm.keys())
    for i in range(len(counties)):
        for j in range(i+1, len(counties)):
            hm2[f'{counties[i]},{counties[j]}'] = distances(counties[i], counties[j])

    with open("counties_distance2.csv", 'w') as f:
        f.write('county1,county2,distance\n')
        for i in hm2:
            f.write(i + ',' + str(round(hm2[i], 5))+'\n')

# Parse the data into more acceptable and readable format since it came as a table
def parse_govt_data():
    overall_array = []
    def parse_row(row):
        co1 = row[0] + row[1]
        co2 = (row[6] + row[7])[-5:]
        if len(co2) < 5: return
        amount = int(row[-2].replace(',',''))
        overall_array.append((co1, co2, amount))
    with open("table3.csv") as f:
        g = csv.reader(f)
        for n,row in enumerate(g):
            if not n: continue
            parse_row(row)

    return overall_array

# Merge the distance and worker in commute traffic data
def merge():
    j = parse_govt_data()
    hm = {}
    with open("counties_distance.csv") as f:
        for n,i in enumerate(f):
            if not n: continue
            g = i.split(',')
            hm[tuple(g[:2])] = g[2]
    with open("edge_data2.csv", 'w') as f:
        f.write("county1,county2,wicf,distance\n")
        for i in j:
            if i[0] == i[1]: continue
            dist = hm.get(tuple(i[:2])) or hm.get(tuple(i[:2][::-1]))
            f.write(f"{i[0]},{i[1]},{i[2]},{dist}")
    
merge()

# Gets the training data, county populations and cleans county repetitions
import csv
import pandas as pd

def get_unique_counties(filename):
    df = pd.read_csv(filename)
    df = df.dropna(subset=['county', 'state'])
    df['county'] = df['county'].str.strip()
    df['state'] = df['state'].str.strip()
    unique_df = df[['county', 'state']].drop_duplicates()
    
    unique_df.to_csv('unique-' + filename, index=False)

get_unique_counties('us-counties-2021.csv')
get_unique_counties('us-counties-2022.csv')
get_unique_counties('us-counties-2023.csv')

def get_county_pop_data():
    with open('co-est2021-alldata.csv', 'r', encoding='utf-8') as input_file, open('county_pop_data.csv', 'w', newline='') as output_file:
        reader = csv.DictReader(input_file)
        writer = csv.writer(output_file)
        
        writer.writerow(['county', 'state', 'population'])
        
        for row in reader:
            county_name = row['CTYNAME'].replace(' County', '').replace(' Parish', '').strip()
            state_name = row['STNAME']
            population = row['POPESTIMATE2021']
            
            writer.writerow([county_name, state_name, population])

def get_county_population(filename):
    county_pop_df = pd.read_csv('county_pop_data.csv')
    unique_counties_df = pd.read_csv(filename)
    merged_df = pd.merge(unique_counties_df, county_pop_df, on=['county', 'state'], how='left')

    population_df = merged_df.dropna(subset=['population'])
    missing_df = merged_df[merged_df['population'].isna()]

    population_df.to_csv('population-' + filename, columns=['county', 'state', 'population'], index=False)

    missing_df.to_csv('missing-population-' + filename, columns=['county', 'state'], index=False)

get_county_population('unique-us-counties-2021.csv')
get_county_population('unique-us-counties-2022.csv')
get_county_population('unique-us-counties-2023.csv')

# Splits the training data into training, validation, testing split
from tqdm import tqdm

def convert_date(line):
    date = line[0:10].replace('-', '')
    new_line = date
    for i in range(10, len(line)):
        new_line += line[i]
    return new_line

def get_training_dataset(data_2021_path, data_2022_path):
    with open('training_dataset.csv', 'w') as f:
        data_2021 = open(data_2021_path, 'r').readlines()
        data_2022 = open(data_2022_path, 'r').readlines()

        for line in tqdm(data_2021):
            f.write(convert_date(line))
        f.write('\n')
        for line in tqdm(data_2022[1:589053]): # 589054 is index to get uo to and including 30-06 in 2022 data
            f.write(convert_date(line))

def get_validation_dataset(data_2022_path):
    with open('validation_dataset.csv', 'w') as f:
        data_2022 = open(data_2022_path, 'r').readlines()

        for line in tqdm(data_2022[589053:]):
            f.write(convert_date(line))

def get_testing_dataset(data_2023_path):
    with open('testing_dataset.csv', 'w') as f:
        data_2023 = open(data_2023_path, 'r').readlines()

        for line in tqdm(data_2023):
            f.write(convert_date(line))

if __name__ == '__main__':
    get_training_dataset('us-counties-2021.csv', 'us-counties-2022.csv')
    get_validation_dataset('us-counties-2022.csv')
    get_testing_dataset('us-counties-2023.csv')